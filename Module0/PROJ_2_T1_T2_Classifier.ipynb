{"cells":[{"cell_type":"markdown","metadata":{"id":"RTyQ_XD8ctVP"},"source":["## Problem definition: Simple classification of Brain images into MRI vs PET\n","\n","\n","In this jupyter notebook, we would like for you to build a basic classification model using convolutional neural netowrks to classify  T1 vs T2 weighted MRI images.\n","\n","In order to enable ease of access we have converted all the DICOM formatted images into JPEG format and randomly sorted them into training and test set. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uI6EXdxMtatL"},"outputs":[],"source":["#Load packages\n","import numpy as np\n","import pandas as pd\n","import os\n","import math\n","import PIL\n","from pathlib import Path\n","import glob as gb\n","from collections import Counter\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import categorical_crossentropy\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","\n","from google.colab import drive"]},{"cell_type":"markdown","metadata":{"id":"wcQnbbl7tyM6"},"source":["Lets mount the drive \n","we have already created a link to the data via shortcuts. The folder are arranged as such: \n","\n","\n","> /content/drive/MyDrive/Module0/PROJ\n","\n",">> /content/drive/MyDrive/Module0/PROJ/TRAIN\n","\n",">>>/content/drive/MyDrive/Module0/PROJ/TRAIN/T1\n",">>>/content/drive/MyDrive/Module0/PROJ/TRAIN/T2\n","    \n",">>/content/drive/MyDrive/Module0/PTOJ/TEST\n","\n",">>>>/content/drive/MyDrive/Module0/PROJ/TEST/T1\n",">>>>/content/drive/MyDrive/Module0/PROJ/TEST/T2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtSa65hsvfQ6"},"outputs":[],"source":["drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOHnbeZi7Onw"},"outputs":[],"source":["train= '/content/drive/MyDrive/Module0/PROJ/TRAIN/'\n","test = '/content/drive/MyDrive/Module0/PROJ/TEST/'"]},{"cell_type":"markdown","metadata":{"id":"jezbYRAXImjX"},"source":["Lets check how many images there are under each label in training and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjTAh9u7Fe6e"},"outputs":[],"source":["num_T1_train_pts = len(gb.glob(train+'T1/*.jpg'))\n","num_T1_test_pts = len(gb.glob(test+'T1/*.jpg'))\n","print('There are {} T1 images in the training folder'.format(num_T1_train_pts)+'\\n'+\n","      'and {} in the test folder'.format(num_T1_test_pts))\n","\n","\n","num_T2_train_pts = len(gb.glob(train+'T2/*.jpg'))\n","num_T2_test_pts = len(gb.glob(test+'T2/*.jpg'))\n","print('There are {} T2 images in the training folder'.format(num_T2_train_pts)+'\\n'+\n","      'and {} images in the test folder'.format(num_T2_test_pts))"]},{"cell_type":"markdown","metadata":{"id":"4k3zGxDGLdID"},"source":["use the function below for plotting the images in the grid."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqayTf_WGNTP"},"outputs":[],"source":["T1_train = gb.glob(train+'T1/*.jpg')\n","T2_train = gb.glob(train+'T2/*.jpg')\n","T1_test = gb.glob(test+'T1/*.jpg')\n","T2_test = gb.glob(test+'T2/*.jpg')\n","\n","\n","def plot_grid(n,files):\n","  # takes argument n of the n x n grid and the list of files\n","  plt.figure(figsize=(8,8))\n","  for i in range(n*n):\n","    plt.subplot(n,n,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    #MRI images \n","    idx=np.random.randint(len(files))\n","    plt.imshow(img.imread(files[idx]),aspect='auto')\n","    plt.xlabel(i)\n","  plt.show()\n"]},{"cell_type":"markdown","source":["Plot T1 and T2 images below using the above function. "],"metadata":{"id":"ESKQAmtGh-iT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZIF_kSvOCnN"},"outputs":[],"source":["#plot_grid(6,T2_test)"]},{"cell_type":"markdown","metadata":{"id":"Xv8Pt6C2UiH-"},"source":["Next, let's load these images off disk as an image-generator object from keras api. The *`ImageDataGenerator()`* class has a convinient function  `flow_from_directory()` utility that can infer the data classes based on the subfolders in the directory. In paractive *`ImageDataGenerator()`* can be used for data agumentation, but we will not be needing this at the moment. \n","\n","**Note** the target size parameter it is set to 256 X 256 which means we are asking Keras to read all the images at that resolution. This is also the dimensions of the first layer in your CNN, so if you would like for the training to run faster or slower you can tweak this parameter. But I reccomend not changing this. \n","\n","We will be splitting images in the train folder into 80% train and 20% validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGwaAG6vUhyJ"},"outputs":[],"source":["train_datagen = ImageDataGenerator(rescale=1.0/255.0,validation_split=0.2) # set validation split\n","classes = ['T1', 'T2'] # class labels\n","image_size=(256,256) #image size\n","train_ds =  train_datagen.flow_from_directory(\n","  train,\n","  subset=\"training\", \n","  seed=123, \n","  classes=classes,\n","  target_size=image_size,\n","  batch_size=32,\n","  interpolation=\"lanczos\")"]},{"cell_type":"markdown","source":["Enter the code block for valdiation data"],"metadata":{"id":"dABWwkvUXXt2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PFLxhmEewvd"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuIz679kf7fx"},"outputs":[],"source":["# Test set will not be shown to the model \n","#it will be used to calculate performance on the trained model\n","test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n","test_ds = test_datagen.flow_from_directory(test, classes=classes,target_size=image_size,batch_size=500)"]},{"cell_type":"markdown","metadata":{"id":"NoILYIYLjdr1"},"source":["These 495 images will be your independent test set which will be used to test our model performance."]},{"cell_type":"markdown","source":["**Use the functions below to get the perfromance of the model**"],"metadata":{"id":"T8I4hL0vhA4Z"}},{"cell_type":"code","source":["def training_performance(history,epochs,metric_list):\n","  ## Takes history and metric_list, returns figure object\n","  metric_list.append('loss')\n","  train_metric = {}\n","  valid_metric = {}\n","  epoch_ranges = range(epochs)\n","  n_metric = len(metric_list)\n","  fig = plt.figure(figsize=(5*n_metric, 4))\n","  j = 1\n","  for i in metric_list:\n","    train_metric[i] = history.history[i]\n","    valid_metric[i] = history.history['val_{}'.format(i)]\n","    plt.subplot(1,n_metric,j)\n","    plt.plot(epoch_ranges,train_metric[i], label='Training_{}'.format(i))\n","    plt.plot(epoch_ranges,valid_metric[i], label='Validation_{}'.format(i))\n","    plt.legend(loc='lower right')\n","    plt.title('Training_Valdiation_{}'.format(i)) \n","    j+=1\n","  return fig\n","\n","def test_performance(y_test,y_pred):\n","  ## Takes test labels and predicted lables returns \n","  ## Returns confusion matrix plot, ROC, PR plots \n","  from sklearn.metrics import classification_report,confusion_matrix, \\\n","                              ConfusionMatrixDisplay, accuracy_score, auc\n","  from sklearn.metrics import roc_curve as roc\n","  from sklearn.metrics import RocCurveDisplay\n","  from sklearn.metrics import precision_recall_curve as pr\n","  from sklearn.metrics import PrecisionRecallDisplay as prd\n","\n","  target_names = ['T1', 'T2']\n","  fig,axes = plt.subplots(1, 3, figsize=(16, 4))\n","  ## Confusion Matrix\n","  cm = confusion_matrix(y_test,y_pred)\n","  print(\"Confusion Matrix \\n \\\n","         \\n T1 \\t {} \\\n","         \\n T1_Predicted_as_T2 \\t {} \\\n","         \\n T2_Predicted_as_T1 \\t {} \\\n","         \\n T2 \\t {} \\n\".format(cm[1][1],cm[1][0],cm[0][1],cm[0][0]))\n","\n","  print (\"Overall Accuracy of the model: {:.3f}\".format(accuracy_score \\\n","                                          (y_test,y_pred)))\n","  cmd = ConfusionMatrixDisplay(cm, display_labels=target_names) \n","  cmd.plot(ax=axes[0])\n","  cmd.ax_.set(xlabel='Predicted', ylabel='Test', \\\n","            title='Confusion matrix' )\n","  cmd.im_.colorbar.remove()\n","  ## ROC curve\n","  fpr, tpr, thresholds = roc(y_test,y_pred)\n","  auroc = auc(fpr,tpr)\n","  roc_plot = RocCurveDisplay(fpr=fpr, tpr=tpr, \\\n","                           roc_auc=auroc,estimator_name='CNN_T1_T2')\n","  roc_plot.plot(ax=axes[1])\n","  roc_plot.ax_.set(xlabel='False Postive Rate', ylabel='True Postive Rate')\n","  roc_plot.figure_.suptitle('ROC')\n","\n","  ## Precision-Recall\n","  precision, recall, _ = pr(y_test,y_pred)\n","  auprc = auc(recall,precision)\n","  pr_plot = prd(precision=precision, recall=recall)\n","  pr_plot.plot(ax=axes[2],name='PR AUC={:.3f}'.format(auprc))\n","  pr_plot.ax_.set(xlabel='Recall', ylabel='Precision')\n","  pr_plot.figure_.suptitle('Precision-Recall Curve')\n","\n","  return fig"],"metadata":{"id":"1tcF7vuwXt3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coRmlmuOAPyH"},"source":["Let's continue with the model..."]},{"cell_type":"markdown","source":["Fill in the model specification"],"metadata":{"id":"LKofCK2piOa4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvstuOpPtgIV"},"outputs":[],"source":["model = Sequential([\n","  layers.InputLayer(input_shape=(256,256,3)),\n","\n","])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"zgHZ5h_HRJ8B"},"source":["Lets look at the summary of the model we specified."]},{"cell_type":"markdown","metadata":{"id":"dJzA-t1hRu0x"},"source":["Let's compile the model. We have to pick an optimizer and loss function to compile the model. For this tutorial, choose the `tf.keras.optimizers.Adam `optimizer and `tf.keras.losses.BinaryCrossentropy` loss function. This loss function is ideal for scienarios where we only have 2 classes in the dataset. To view training and validation accuracy for each training epoch, pass the metrics argument to `Model.compile`. You can read more about these options here:\n","\n","> https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam \n","> https://www.tensorflow.org/api_docs/python/tf/keras/losses/\n","\n","These are also some parameters you can play with to see how the performance of the model changes. Finally the metrics parameter specifies how we want to frame the problem. Recall the introduction. Lets pick accuracy which is a balance between both the classes."]},{"cell_type":"markdown","source":["Compile the model with either adam or SGD optimizer and report the perfromance"],"metadata":{"id":"3tGgZbYmiVbE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPSQLKs7tp-b"},"outputs":[],"source":["num_steps_per_epoch = 60\n","num_validation_steps = 20\n","num_epochs = 10\n","\n","### Refer PET MRI classifier if in doubt\n","model.compile()"]},{"cell_type":"markdown","metadata":{"id":"I_Isubv-R4YY"},"source":["Train the model in 10 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMKtnkfft3VG"},"outputs":[],"source":["history = model.fit(\n","  train_ds,\n","  steps_per_epoch=num_steps_per_epoch,\n","  validation_data=valid_ds, \n","  validation_steps=num_validation_steps,\n","  epochs=num_epochs, verbose=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"Dy8s8UcIWBbB"},"source":["Lets check how the model has trained. We can do this by plotting the accuracy between traing data and validation data"]},{"cell_type":"code","source":["test_imgs, test_labels = next(test_ds) "],"metadata":{"id":"qI5VJJPxTSmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict(x=test_ds, verbose=1)"],"metadata":{"id":"1UMityzhTyqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loss, test_acc = model.evaluate(test_ds,\n","                                     steps=1)\n","                                     #verbose=0) \n","print(test_loss)\n","print(test_acc) "],"metadata":{"id":"uW7JMgSRUCZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kfq8iS0juhTQ"},"outputs":[],"source":["y_test = test_labels[:, 0]\n","y_pred = np.round(predictions[:,0])\n","a = training_performance(history,num_epochs,['accuracy'])"]},{"cell_type":"code","source":["b = test_performance(y_test,y_pred)"],"metadata":{"id":"FEiEk_hHdmT9"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"PROJ_2_T1_T2_Classifier.ipynb","provenance":[],"authorship_tag":"ABX9TyNJ91S3uaqrnQVLaeJBf2vt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}